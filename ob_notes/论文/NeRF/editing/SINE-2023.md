# SINE：Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field

>在不改变NeRF的情况下，将**编辑部分用新的网络来学习**，即学习到新的位置和新的颜色，并通过最终的合成来得到编辑后的图像

![[Pasted image 20230825114232.png]]

## 1. 网络结构与训练过程

### 1. 网络结构

主要包括三个大块：Input内容、Editing and Template Network、合成最终图像

* Input包括预训练完成的NeRF、一张编辑后的2D图像（可以是用户编辑也可以是text-promt生成）
* Network部分除了主要的Template NeRF，还有前面添加了的Editing Field，用来得到编辑后的点坐标 $\textbf{x}'$ 和纹理颜色 $\textbf{m}'$
* 合成部分先分别得到经过只几何编辑得到的RGB图像 $\hat{I}_o$ 和只经过纹理编辑得到的masks中的RGB图像 $\hat{I}_m$，再将二者通过Color Compositing Layer的CNN网络合成最终编辑的图像 $\hat{I}$ （下图为 $\hat{I}_o$ 的color值 $\hat{C}_o$ 和 $\hat{I}_m$ 的color值 $\hat{C}_e$）![[Pasted image 20230825131647.png]]

### 2. 训练过程

下面简单描述一下训练过程（其中1为预训练，2，3为编辑后的训练过程（里面可能会包含其他的需要预训练的东西））

1. 得到预训练的NeRF（之后的过程中不用再更新参数）
2. 单独训练Geometric Modification $F_{\Delta G}$
3. 训练Texture Modification $F_{\Delta T}$ 和 Color Compositing Layer

## 2. Prior-Guided Geometric Editing 先验指导几何编辑

> 为了防止**单视角GT图片提供的信息不足**，导致学习到的网络出现**几何上的ambiguity**，所以采用了Prior-Guided来得到**先验的完整几何模型**来指导网络学习

![[Pasted image 20230825091220.png]]

### 1. Prior-Guided Geometric Editing

为了**得到一个较为准确得Shape prior $\hat{M}_p$ 显示网格表示**来约束Geometric Modification $F_{\Delta G}$ 来学习生成更准确的几何，paper中根据物体的类型采用了两种方法：

1. 如果objects在特定的类别中，那么就可以使用**预训练好的**[DIF](https://blog.csdn.net/qq_40142891/article/details/117224888)来得到编辑后的较为准确的Shape Prior $\hat{M}_p$
	* 上图中的 $\hat{z}$ 为预训练好的（在得到预训练的NeRF之后预训练）对应着具体物体编辑前的几何的latent code，可以直接输入到DIF的网络中得到 $\hat{M}_p$；当需要编辑物体的时候再重新更新 $\hat{z}$ 来的到对应的 $\hat{M}_p$
2. 如果objects不在这些已经训练好的类别中，那么就需要**预先得到物体的显示几何表示**，再通过一些**对显示几何编辑的方法**来得到 $\hat{M}_p$（这里有点像[[NeRF-Editing-2022]]中的显示几何表示方法）

将Deformed Proxy Mesh $\hat{M}_E$ 变形后的网格表示与 Shape Prior $\hat{M}_p$ 先验几何表示二者尽量相似得到损失函数 $L_{gp}$（第一项前面表示让编辑后的物体表面的点 $\textbf{p}'$ 尽量落在先验的几何表面上，即SDF为0，后面为latent code的正则项；第二三项为了让两个几何的表面点接近）

![[Pasted image 20230825092548.png]]

### 2. Representing edited NeRF's Geometry as a deformed proxy mesh

为了得到**编辑后的显示几何表示Deformed Proxy Mesh $\hat{M}_E$**：用[NeuS](https://zhuanlan.zhihu.com/p/588870246)得到Template NeRF（编辑前）的代理几何网格Proxy Mesh $M_\Theta$，然后通过学习一个**Forward Modification 网络 $F_{\Delta G}'$ 来得到变形后的显示几何表示 Deformed Proxy Mesh** $\hat{M}_E$（$F_{\Delta G}'$ 其实就是几何变形网络 $F_{\Delta G}$ 的逆过程，给定Mesh的点坐标，得到变形后的点坐标）。得到了显示集合表示 $\hat{M}_E$ 后就可以用[[#1. Prior-Guided Geometric Editing]]中的先验指导来学习几何网络 $F_{\Delta G}$
 
这个过程需要学习Forward Modification 网络 $F_{\Delta G}'$，可以用循环损失来监督 $L_{cyc}$（即让正反输出结果尽量一样）

![[Pasted image 20230825132409.png]]

### 3. Learning geometric editing with users' 2D editing

当然，除了先验指导，用户还给了修改后的2D的GT图像，所以也需要**计算与GT的损失** $L_{gt}$：包括**RGB图像的损失**（左边部分）和**不透明度的损失**（右边部分），其中不透明度的真实值 $O_e(\textbf{r})$ 是用户编辑物体时候的编辑软件中的 silhouette 剪影

![[Pasted image 20230825092536.png]]

同时为了让几何在空间中**更平滑**以及**减轻过拟合网格表面**的情况，添加了下面的**正则化损失** $L_{gr}$：包括防止梯度过大（左边）和鼓励相邻位置变化更平缓（右边）

![[Pasted image 20230825093028.png]]

### 4. 最终所有损失

![[Pasted image 20230825093237.png]]

## 3. Prior-Guided Texture Editing

> 同几何学习一样，为了防止**单视角GT图片提供的信息不足**，导致学习到的网络出现**纹理的不完整**，所以采用了**semantic texture prior语义纹理先验**来让纹理学得更好

纹理的编辑**需要的是一张目标图** $I_t$（训练的时候也只需要这一张target图即可），这张图可以通过**用户直接编辑得到**，也可以与一些 **text-promts editing methods 基于文本生成的方法**来得到目标图像（这里用的文本生成图像的方法是[Text2LIVE](https://zhuanlan.zhihu.com/p/570882311)）

![[Pasted image 20230825095958.png]]

### 1. Semantic texture prior supervision

有了用户给的编辑后的图像 $I_t$ 以及 经过网络得到的两个图像 Template image原版图像 $\hat{I}_o$ 和 Edited image合成的编辑后的图像 $\hat{I}$ 后，运用**pre-trained的 [DINO-ViT](https://zhuanlan.zhihu.com/p/440247722) 模型**来集中注意到需要编辑的物体，通过[Splicing ViT Features](https://arxiv.org/abs/2201.00424)中计算损失的方法来使得图像 $I_t$ 和预测的编辑后图像 $\hat{I}$ 有相似的image cues（应该是**相似的图像纹理特征**）（式子左边）以及让编辑后的图像 $\hat{I}$ 和编辑前的图像 $\hat{I}_o$ 有**相似的spatial structure空间结构**（式子右边）

![[Pasted image 20230825095942.png]]

前半部分为了让给定；后半部分为了

### 2. Decoupled rendering with color compositing layer

为了得到**合成的编辑后的图像** $\hat{I}$，最简单的方法是在**体渲染的时候直接将修改后的颜色 $\textbf{m}'$ 加入到当前NeRF预测的颜色 $\textbf{c}$ 中**，但是这样会导致陷入**局部最优**情况

于是这里先得到两个图像：Deformed Template几何变换后的图像 $\hat{I}_o$ 和 Color Modification颜色变换后的图像 $\hat{I}_m$，二者进行**2D的CNN-based的color compositing layer**来得到最终编辑后的图像 $\hat{I}$。因为coarse semantic supervision粗糙的语义监督不容易学到fine-grained details好的细节，但是CNN层可以**较容易地解决这些问题**（怎么学得到？？），同时也可以**学到view-dependent effrcts**来使得渲染更加真实

![[Pasted image 20230825110623.png]]

## 4. Editing Regularization

> 为了能够更加准确地修改需要修改的区域，首先需要**得到准确的masks**，然后通过加入正则项来**保持非masks区域不变**

### 1. Feature-cluster-based semantic masking

借助[DINO-ViT](https://arxiv.org/abs/2104.14294)方法来学习distilled feature field，并根据用户编辑的 silhouette $M_e$ 剪影来从distilled frature map中产生多个feature clusters（这里应该是为了得到得到多个聚类），然后使用到cluster中心的cosine相似度来计算**得到semantic masks** $\hat{M}_e$ 掩码（应该是为了得到与mask最接近的那个聚类来得到最终的masks包的部分）（ViT这一块不是很理解，有需要再来补吧）

### 2. Regularization on geometric and texture editing

有了semantic masks后，为了**让无关的区域的几何和颜色不变**，引入了下面的损失计算 $L_{reg}$（$\in\hat{I}\backslash\hat{M}_e$前面表示不在masks $\hat{M}_e$内的区域，$\hat{C}(\textbf{r})$ 为最终的合成的编辑后的图像，$\hat{C}_o(\textbf{r})$ 为Deformed Template $\hat{I}_o$ 的RGB图像；前半部分表示让点位置 $\textbf{x}$ 不变（$F_{\Delta G}(\textbf{x})$ 表示位置偏移？），后半部分表示让点颜色不变）

![[Pasted image 20230825113759.png]]

## 5. Contributions & Limitations

### 1. Contributions

1. 在场景编辑上，其他方法有的直接修改主网络（[[Edit NeRF-2021]]、[[NeRF-In-2022]]），有的通过一些映射关系将当前点映射回原本的点中而不修改主网络（[[ST-NeRF-2021]]、[[NeRF-Editing-2022]]、[[DM-NeRF-2023]]），这里采用的方法与后者类似，**不修改主网络NeRF**，但是也不是直接得到显示的映射关系，而是**通过一个网络来学习几何和纹理的映射/变化关系**
2. 为了减少只有一张编辑后的GT图带来的ambiguity影响，运用了**Prior-Guided来监督几何和纹理的学习**
3. 得到较**准确的masks**来防止学到masks外的部分

### 2. Limitations

1. 只允许物体的几何变形和颜色编辑，**不能够进行移动、旋转、移除等操作**