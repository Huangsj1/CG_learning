# Editing Conditional Radiance Fields

> 通过为每个物体单独使用latent code来表示其形状$\textbf{z}^{(s)}$和颜色$\textbf{z}^{(c)}$特征，然后可以修改网络/latent code来修改颜色、编辑物体（remove/add）

![[Pasted image 20230821163545.png|400]]

## 1. 网络结构

![[Pasted image 20230821163615.png|500]]

同NeRF一样包括coarse和fine网络，网络主要包括三部分：

1. 场景的共享空间信息 $F_{share}$ 和每个物体的几何信息 $F_{inst}$
2. 物体在场景中的空间位置信息 $F_{fuse}$ 和根据该信息通过网络 $F_{dens}$ 得到体密度 $\sigma$
3. 通过 $F_{fuse}$ 得到的物体在场景中的颜色信息 $\textbf{c}$

## 2. 网络的训练

类似于 NeRF 中将位置信息 $\gamma(\textbf{x})$ 传入到前面网络中得到体密度 $\sigma$，再传入方向信息 $\gamma(\textbf{d})$ 传入到后面的网络中得到radiance $\textbf{c}$。只不过这里在前面网络通过增加一段网络 $F_{inst}$ 来对每个物体形状latent code $\textbf{z}^{(s)}$ 解码得到该物体的几何信息；在后面的网络中通过和view direction $\gamma(\textbf{d})$ 一起传入每个物体的颜色latent code $\textbf{z}^{(c)}$ 来得到该物体的颜色信息

**准备工作**：

1. 需要得到场景中所有物体实例 $k\in\{1,...,K\}$ 所包含的所有光线 $R_k$，
2. 同时初始化每个物体的几何latent code $\textbf{z}_k^{(s)}$ 和颜色latent code $\textbf{z}_k^{(c)}$

**训练流程**：

1. 对每个物体 $k$ 所包含的所有光线 $R_k$ 进行采样得到采样点
2. 将采样点的位置信息 $\textbf{x}$、方向信息 $\textbf{d}$、几何latent code $\textbf{z}_k^{(s)}$、颜色latent code $\textbf{z}_k^{(c)}$ 传入到网络中得到体密度 $\sigma$ 和 radiance $\textbf{c}$
3. 通过体渲染方程得到光线颜色，与GT中进行损失计算![[Pasted image 20230821165730.png|400]]

## 3. 编辑操作

这些编辑操作都是通过**图形界面来操作**的，用户通过随意图画选择需要编辑的物体，然后会通过一些算法（没有提什么方法）来得到对应的需要编辑的部分。这里主要讲的是**网络是如何重新训练的**

latent code 和网络的个人理解：**latent code**记录着每个物体 *特有的本质的* 一些几何和颜色信息；**网络**记录着物体在场景中 *表现的* 相关信息（如位置、光照、可见等），决定了物体是如何展现的

1. 只更新latent code（图a）：效果很差，因为在场景中只修改了物体的颜色/几何后，在网络中可能未能根据修改了的latent code来重新展现物体
2. 只更新网络（图b）：效果也很差，因为物体的一些本质特征没有改变
3. latent code 和 网络一起更新：效果较好，但是依然会出现一点artifacts（估计是二者同时更新有点过拟合，导致其他视角没能很好地拟合）
4. 只更新后面的网络 $F_{fuse}$ 和 $F_{dens}$：效果好，应该是物体的一些本质属性没有变（或许latent code包含的是一些基础的大体几何、颜色信息），而后面网络学习的是如何将物体的哪些部分展示出来，所以效果较好（但是感觉这样解释说服力不足，他们估计也是控制变量得到这样更好）

![[Pasted image 20230821170213.png|200]] ![[Pasted image 20230821170223.png|200]] ![[Pasted image 20230821170242.png|200]]

![[Pasted image 20230821170543.png]]

下面的三种编辑操作（颜色编辑、形状去除、形状添加）都是需要用户来选择需要修改的部分，需要**修改的部分为foreground**，**不需要修改的部分为background**（需要有二者的masks，具体是怎样根据用户的草图得到的没有讲，或许是像套索选择工具一样选择一整块？）

### 1. Color Edits

$\textbf{c}_f$ 为foreground中需要修改后的颜色，$\textbf{c}_b$ 为background中不需要修改的颜色；$y_f\{(\textbf{r},\textbf{c}_f)\}$ 为用户选择的需要修改的foreground中的光线 $\textbf{r}$ 和所对应的当前像素值的组合，$\textbf{c}_b$ 为用户选择的不需要修改的background中的光线 $\textbf{r}$ 和所对应的当前像素值的组合，损失函数的计算就需要计算这些光线当前预测的像素与期望的像素的颜色差$L_{rec}$（还包括正则项$L_{reg}$）。其中需要梯度下降更新的只有物体颜色的latent code $\textbf{z}^{(c)}$和部分网络$F_{rad}$

![[Pasted image 20230821180454.png]]
![[Pasted image 20230821180640.png]]

### 2. Shape part removal

对于需要删除的部分 $\textbf{r}\in y_f$，期望的颜色为白色（这里假设没有遮挡）除了像[[#1. Color Edits]]中的像素值差异$L_{rec}$，还需要让每个**采样点的体密度尽量稀疏**（$L_{dens}$让体密度值更小），使得模型预测该方向的体密度为0

![[Pasted image 20230821181156.png]]
![[Pasted image 20230821181202.png]]

### 3. Shape part addition

这里的添加物体应该就是添加一个新的object到原本场景的所有物体中。论文所讲的是使用**新物体的shape code**以及**原本物体的color code**来进行训练（应该就是复制一份），但是论文没有将在这里添加了之后是否需要修改原本物体在该位置的color（我觉得应该要吧）

![[Pasted image 20230821183745.png]]

### 4. Color and Shape Transfer

除了上面的操作外还可以迁移物体的Color 和 Shape，只需要交换latent code即可。这也表明了Color 和 Shape的latent code真的能学到物体的一些本质信息（几何、颜色），而网络只是让它们如何展示的（包括展示位置、展示的部分）

![[Pasted image 20230821181617.png]]

## 4. 优化

1. **Subsampling**：对用户选择需要修改的区域的光线进行随机采样 ，而不是全部采样，这样不仅能加快速的，还能防止一些不能修改的背景被修改
2. **View direction dependence**：因为对于选择需要编辑的物体只有一个方向的视图，可能会导致其他方向的视图颜色不一致，所以通过对同一个位置的各个方向的预测颜色值进行正则化，让各个方向的颜色趋于相似![[Pasted image 20230821173733.png]]![[Pasted image 20230821173746.png]]
3. **Feature caching**：对于训练好的网络，再修改的时候只会修改部分网络，而对于没有修改的部分的值可能不变（采样点不变的时候$F_{share}$和$F_{inst}$输出不变，修改颜色时的$F_{fuse}$也不变），所以可以通过存储这些值就算过的结果来使得后面再次训练时加快速度（比如编辑颜色时，体密度$\sigma$不变，采样的点也不便，所以可以直接记录第一次计算的$F_{rad}$之前的值，这样后面就可以直接使用）


## 5. Contributions & Limitations

### 1. Contributions

1. 通过将场景中的物体分开，得到每个物体的几何和颜色的latent code $\textbf{z}_k^{(s)},\textbf{z}_k^{(c)}$，然后需要修改的时候就可以**单独修改该物体**
2. 可以通过不断选择更新部分网络或者latent code来调出修改哪一部分更好

### 2. Limitations

1. 对于网络更新的**解释性不够**，都是根据不断调节得出最好的结果
2. 当有**物体覆盖**的时候做的不够好

# 疑惑

1. 用户简单的涂鸦选择需要修改的区域，那是**如何得到整片相关区域的mask的呢**，还是说只用将用户涂鸦的那部分作为mask来修改
2. 添加物体是直接添加一个新的object到所有物体中吗，还是只是修改原本的物体（他这里给的演示图片和视频都只是为原本的物体填充一些缺失的部分，没有展示添加其他物体）