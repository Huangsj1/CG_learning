# NeRD：Neural Reflectance Decomposition

![[Pasted image 20230815164152.png]]

## 1. motivation

1. **解耦**体渲染方程得到 *光照信息*、*BRDF信息*（包括Base color材质基本颜色、Metallic金属度、Roughness粗糙度）、*法线信息* 等，之后就可以重光照或者其他操作（inverse rendering）
2. [[NRF-2020]]中需要光源和相机同一位置且知道光源值，[[NeRV-2020]]中需要知道光源位置和值，这些方法都**需要知道光源信息**。于是这里用了SG（Spherical Gaussian）[球面高斯函数](https://zhuanlan.zhihu.com/p/514017351)来学习每张照片的光照信息（也就是输入的光照条件可以不同，类似于W-NeRF中）

## 2. procedure 过程

总体分为两个大网络**Sampling Network** 和 **Decomposition Network**

### 1. Sampling Network

>主要负责提供采样点权重信息给后者采样

![[Pasted image 20230815164604.png|400]]

类似于[[Wild NeRF-2021]]中的利用了光照的latent code的网络

1. 第一个网络$N_{\theta_1}$ 同NeRF/W-NeRF中网络的前面部分一样，都是为了学习得到采样点的体密度 $\sigma$
2. 第二个网络$N_{\theta_2}$ 为了得到类似于W-NeRF中的光照信息latent code，为**每个image j 提供一个 $\pmb\Gamma^j$ 包含多个球面高斯函数来学习其光照信息**，将 $\pmb\Gamma^j$ 压缩成 latent code。（类似于傅里叶变换和泰勒展开，这里用多个（这里用24个）球面高斯函数 $G(\pmb\omega_i;\pmb\mu,\lambda,\textbf{a})=\textbf{a}e^{\lambda(\pmb\mu\cdot\pmb\omega_i-1)}$ 来近似得到各个方向的光照值（每个球面高斯函数需要7个参数：3维$\pmb\mu$表示波瓣中心方向/入射光方向，1维$\lambda$表示波瓣胖瘦，3维$\textbf{a}$表示波瓣大小/入射光强度））
	* 注意这里的 $\pmb\Gamma^j$ 是不进行梯度更新的，而是在Decomposition Network才进行更新
	* 这里的 $\pmb\Gamma^j$ 是能够直接参与rendering计算的，学得更**具体**，但是W-NeRF学的只是照片的光照特征，是作为网络输入的
3. 第三个网络$N_{\theta_3}$ 将经过网络$N_{\theta_2}$浓缩成latent code 和 网络$N_{\theta_1}$ 得到的一层输出作为输入，输出每个采样点的预测颜色 $\textbf{c}_{\textbf{x}_i}^j$ 
4. 通过体渲染方程 $C(\textbf{r}) = \int_{t_n}^{t_f}T(t)\cdot\sigma(\textbf{r}(t))\cdot\textbf{c}(\textbf{r}(t),\textbf{d})dt$ 得到每根光线的color值 $\textbf{c}_r^j$
5. Loss为预测的图像与真实图像每个像素的L2损失 **MSE**：$\frac{1}{s}\sum^s(\widehat{\textbf{c}}_r^j-\textbf{c}_r^j)$

### 2. Decomposition Network

>主要负责学习提取体密度 $\sigma$、BRDF信息$\textbf{d}$、法线信息$\textbf{n}$、光照信息$\Gamma$

![[Pasted image 20230815171337.png|500]]

1. 第一个网络 $N_{\phi_1}$ 同NeRF中的fine网络一样以第一个网络的采样点的权重来再次采样，经过PE编码后输入到网络中，输出包括：体密度$\sigma_{\textbf{x}_i}$、direct color直接光照颜色$\textbf{d}_{\textbf{x}_i}$、一层输出（供下一层网络使用）。其中输出的**体密度 $\sigma_{\textbf{x}_i}$ 会通过取负梯度得到法线** $\textbf{n}_{\textbf{x}_i}$ （如果通过网络再输出法线，会导致二者冲突）
	* 体密度的梯度是体密度增加最快的方向，即从最小往最大的方向；法线是垂直于物体表面指向外面，也可以理解成从体密度最大往最小变化的方向；所以法线可以用体密度的负梯度来表示
2. 第二个网络 $N_{\phi_2}$ 将上一层网络的一层输出经过encoder编码成2维向量（因为实际生活中物体在**相邻区域通常会有相同的属性**，将这些属性进行压缩使得这些学习时学到的都近似相同，使得更有robust），再decoder成5维向量$\textbf{b}_{\textbf{x}_i}$（BRDF属性：3维Base color基本颜色、1维金属度Metallic、1维粗糙程度Roughness）
3. 到现在为止得到的全都是 *采样点* 的各种信息，中间 "Comp." 的作用是通过**每个采样点的 $\alpha$ 权重来将所有采样点的属性累加起来**（就像 Sampling Network中求像素的颜色一样将所有采样点的颜色根据权重加和起来），最终得到当前 *光线* 的属性值：direct color直接颜色 $\textbf{d}_r$、法线 $\textbf{n}_r$、BRDF属性 $\textbf{b}_r$。这有点像将物体当作是hard硬表面，然后取其中期望深度的点作为与该光线的交点（类似[[NeRV-2020]]中取期望的接收间接光照的点）
	* 个人理解BRDF的 $\textbf{d}_r$ 中的base color只有三个维度，学到的物体基本颜色属性**有限**，从而需要 $\textbf{d}_{\textbf{x}_i}$ 学到剩下的一些 illumination independent 的**共享**的直接光照信息（**特有**的光照信息被 $\pmb\Gamma_m$ 学走）
	* 这里取所有采样点的期望属性（根据 $\alpha$ 权重来加和所有采样点的属性），可以从硬表面取交点来理解，也可能是因为对所有采样点都根据BRDF来求一次  $$L_o(\textbf{x},\pmb\omega_o)=\int_\Omega f_r(\textbf{x},\pmb\omega_i,\pmb\omega_o)\ L_i(\textbf{x},\pmb\omega_i)\ (\pmb\omega_i\cdot\textbf{n})\ d\pmb\omega_i\tag{1}$$再带入体渲染方程会导致**计算量很大**，所以取一个期望点来计算上式结果作为像素值
4. “Render”中使用[球面高斯函数的替换](https://dl.acm.org/doi/10.1145/1618452.1618479)来来计算得到像素点的color $\textbf{c}_{\omega_{or}}^j$：$$L_o(\textbf{x},\pmb\omega_o)\approx\sum_{m=1}^{24}\rho_d(\pmb\omega_o,\pmb\Gamma_m,\textbf{n},\textbf{b})+\rho_s(\pmb\omega_o,\pmb\Gamma_m,\textbf{n},\textbf{b})\tag{2}$$前面部分表示diffuse漫反射的值，后面部分表示specular镜面反射的值。$\pmb\Gamma^j$中的每一个球面高斯函数都存储着当前image的光照信息。最终能够得到像素点的 $\textbf{c}^j_{\omega_{or}}$ 
5. Loss计算根据论文描述是通过最终输出的像素值 $\textbf{c}^j_{\omega_{or}}$ 与GT像素值 $\widehat{\textbf{c}}_r^j$ 作L2损失，同时加上像素点的直接颜色 $\textbf{d}_r$ 与GT的 $\widehat{\textbf{c}}_r^j$ 进行损失计算，但是这个直接颜色会进行指数衰减，因为其与真实图像还是有点不同，权重不应该过高

## 3. contribution & limitations

### 1. contribution

1. 用**球面高斯函数SG来作为可优化的输入参数**学习每张不同光照信息的图片的**光照信息**（类似于W-NeRF中的latent code）
2. 通过**取采样点的各种属性的期望值**来作为一个硬表面交点，计算其BRDF的rendering的像素值，简化计算

### 2. limitations

1. 需要对所需要的物体用mask拿出来
2. 没有考虑**间接光照**
3. relighting的话需要重新根据BRDF和法线完整地光线追踪

## 4. 疑惑

1. 为什么Decomposition Network中需要得到直接光照的值 $\textbf{d}_r$，物体基本光照属性不是在 BRDF的 $\textbf{b}_r$ 中有学习吗
2. 为什么需要取所有采样点的期望点的属性来通过BRDF方程得到最终像素点的颜色，为什么不对每个采样点直接求BRDF方程，然后带入到体渲染方程中，是因为时间问题吗