# 一、NeRF （Neural Radiance Field 神经辐射场）

## 1. motivation 动机

### 背景

新视角合成方法通常可以使用一个**中间3D场景表征**来作为中介生成高质量的虚拟视角，通过该3D场景就可以渲染出不同视角下的2D图片，而表征方法又分为“显示表征”和“隐式表征”。

1. “**显示表征**”3D场景包括 **Mesh网格**、**Point Cloud点云**、**Voxel体素**等 *离散* 表示，直接得到**显示**的位置 / 结构等信息
	* 优点：能够对场景显示建模，之后能进行更多操作
	* 缺点：离散表示不够精细化造成重叠等伪影；需要存储的三维场景信息数据量大，内存需要高，限制了高分辨率场景的使用
2. “**隐式表征**”包括**Occupancy占用**、**Signed Distance Function (SDF) 符号距离函数**、**NeRF神经辐射场**等 *连续* 表示，通过**神经网络来近似隐函数**的表示方法，将复杂度3D场景信息存储到神经网络的参数中
	* 优点：容易学习；因为其是连续的表达方式，所以可以渲染出更大分辨率的场景
	* 缺点：需要将网络得到的信息重建后显示表示；需要对每个像素都经过神经网络处理，计算量大，耗时

![[Pasted image 20230802113516.png]]

### 动机

NeRF基于**体渲染**的方法（需要知道物体的粒子信息，如密度等），通过对光线等间距 $\delta_i$ 采样，在知道了粒子密度 $\sigma_i$、采样点的颜色 $c_i$（包括自发光、外面弹射进来的光线等）、透射比 $T_i$（由沿着人眼发出光线方向对粒子密度积分得到）之后，就可以通过**离散化的体渲染方程**得到像素值，就可以得到整张图片。由于上述需要知道的信息 $\sigma_i$、$c_i$ 都太过复杂，于是就想通过**神经网络来学习得到这些信息**，之后通过体渲染还原得到的图像与GT相比来计算损失实现梯度下降更新参数，使得学习$\sigma_i$、$c_i$ 更加准确，还原出更加符合的图像

## 2. 实现过程

* 步骤：
	1. 对每个像素发出光线，**采样得到一系列的点**（取不同的 t 得到不同的位置）
	2. 将点的**三维position**和**二维viewing direction**输入到**MLP**中得到 color 和 density输出
	3. 根据color 和 density 利用**volume rendering体渲染**得到生成的二维图像，将该图像与GT图像比较计算L2损失，并利用梯度下降更新MLP

![[Pasted image 20230802104706.png]]

### 1. 采样

对所有的光线进行 stratified sampling **分层采样**（每个光线采样多个点）：

$t_i \sim U[t_n + \frac{i-1}{N}(t_f-t_n)),t_n+\frac{i}{N}(t_f-t_n)]$

将 $[t_n, t_f]$ 分成 N 个均匀等间隔的箱子，每个箱子内随机抽取一个样本

### 2. MLP 全连接层

**全连接层MLP**：$F_\theta : (x, d) \rightarrow (c, \sigma)$ ，其中用8层全连接层+1层全连接层来将position **x** 作为输入得到density **$\sigma$** 和一个256维的vector输出（因为体密度 $\sigma$ 与view direction无关，只与position **x** 有关），再用两层全连接层来将前面得到的vector和view direction **d** 作为输入得到color **c** 输出

* 注意：这里会将view direction **$d(\theta, \phi)$** 转化为笛卡尔坐标系 $d(x', y', z')$

![[Pasted image 20230801145654.png]]

![[Pasted image 20230801211320.png]]

### 3. Volume Rendering 体渲染

将上面神经网络得到的color **c** 和density **$\sigma$** 作为输入，经过下面的**体渲染方程**得到预测的图像后，将该图像与gt比较，计算L2_loss并进行梯度下降来更新神经网络

* Volume Rendering 体渲染方程：$C(\textbf{r}) = \int_{t_n}^{t_f}T(t)\cdot\sigma(\textbf{r}(t))\cdot\textbf{c}(\textbf{r}(t),\textbf{d})dt$ ，其中 $T(t) = exp(-\int_{t_n}^{t}\sigma(\textbf{r}(s))ds)$ ^23-8-1-volume-rendering
	1. $\textbf{r}(t) = \textbf{o} + t\textbf{d}$ 为光线在 t 时刻的向量
	2. $C(\textbf{r})$ 为光线 $\textbf{r}(t)$ 所得到的**当前像素的color**值
	3. $T(t)$ 为光线从 $t_n$ 到 t 的**累计透射率**，即光线从 $t_n$ 到t传播而不撞击任何其他粒子的概率，他取决于体密度 $\sigma(\textbf{r}(t))$
	4. $\sigma(\textbf{r}(t))$ 为当前光线位置的**体密度 volume density**
	5. $\textbf{c}(\textbf{r}(t), d)$ 为当前光线位置的由**MLP预测的color值**（一个三维坐标+方向的color）
* 根据离散采样将体渲染方程改写成离散形式：$\hat{C}(\textbf{r}) = \sum_{i=1}^{N}T_i\cdot(1-exp(-\sigma_i\delta_i))\cdot\textbf{c}_i$，其中 $T_i = exp(-\sum_{j=1}^{i-1}\sigma_j\delta_j)$ ^23-8-1-discrete-volume-rendering

## 3. 优化

### 1. Positional encoding 位置编码

* 问题：
	1. 因为深度神经网络中**偏向学习低频函数**——低频原则（低频信号对梯度的贡献大于高频，所以梯度下降倾向于消除低频误差）
	2. 且输入的 **x** 和 **d** 都是**低维的**（总共就5维），对于高频变化明显的地方，输入的x和d变化都不是很大，导致神经网络较难去学习。
	3. 所以最后训练得到的图像在**高频上面有损失**（图像变化明显的地方，如一些微小的颜色、几何变化损失了，显得到处都很平滑）

* 方法：通过[Rahaman等人的研究](https://arxiv.org/abs/1806.08734)：将输入先通过**高频函数进行映射到高维空间**中（使得原本小的变化在高维空间中都是比较大的变化，易于神经网络去学习），再传入到神经网络中能够使得数据包含高频部分

* 具体做法：对 **x** 中的三维坐标和 **d** 中的二维坐标都经过映射变成高维：

$\gamma(p) = (sin(2^0\pi p), cos(2^0\pi p), ···, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))$ 

其中 **x** 的 L 取10（**x** 中的每一个维度都变成 L=20 个维度，所以前面的 $\gamma(x)$ 为3 * 20 = 60维），**d** 的 L 取4（d 中的每一个维度都变成了 L=8 个维度，所以后面的 $\gamma(x)$ 为3 * 8 = 24维）

![[Pasted image 20230801151925.png]]

上图中最右边是没有经过 Positional Encoding 优化的图像（损失了细节变化），第二张图是完整的模型结果

### 2. Hierarchical volume sampling 启发式体积采样

* 问题：在对光线中的点采样的时候，由于采样的点分布较均匀，这就可能导致有很多地方是 free space空闲 and occluded遮挡的，这些点对结果 C 贡献不大，但是依然重复采样

* 方法：通过样本对 final rendering 的影响**按比例分配样本**来提高效率

* 具体做法：将[[#^23-8-1-discrete-volume-rendering|离散形式的体渲染方程]]改写成 $\hat{C}_c(\textbf{r})=\sum_{i=1}^{N_c}\omega_i\cdot c_i$ ，可以将 $\omega_i$ 看成是采样点 i 的color的权重，将 $\hat{\omega}_i = \frac{\omega_i}{\sum_{j=1}^{N_c}\omega_j}$ 归一化生成piecewise-constant片状恒定的pdf（概率密度函数）。建立两个神经网络“coarse”和“fine”，先进行**粗粒度采样**（左图）——同之前一样一条光线采样 $N_C$ 个点，经过第一个神经网络“coarse”后输出 $\sigma_i$ 和 $c_i$ ，计算得到权重 $\hat{\omega}_i$ ，然后再进行**细粒度采样**（右图）——进行逆变换后在cdf纵坐标上均匀采样$N_f$ 个点（下图），根据这 $N_c+N_f$ 个点输入到第二个神经网络“fine”中得到 $\sigma_i$ 和 $c_i$ 输出，再经过体渲染方程得到最终的 $\hat{C}_f(\textbf{r})$ 。，权重更大的点的位置贡献更大，更容易再次采样。最后的损失函数：$L = \displaystyle\sum_{r\in R}[\|\hat{C}_c(\textbf{r})-C(\textbf{r})\|_2^2 + \|\hat{C}_f(\textbf{r})-C(\textbf{r})\|_2^2]$ 

![[Pasted image 20230801212414.png|300]] ![[Pasted image 20230801212545.png|300]]

![[Pasted image 20230801212459.png]]

#### 逆变换采样

将pdf（概率密度函数）累加起来得到cdf（累计密度函数），通过在 \[ 0, 1 ]随机采样得到 x，传入函数 $cdf^{-1}(x)$ 得到的结果就是符合概率密度pdf 的值（相当于上图中取y 轴上的点得到x 轴上的值）

具体到实现中因为不知道函数样子也就无法求 $cdf^{-1}(x)$，且$cdf(x)$ 也是离散的，于是可以通过在 $cdf(x)$ 中进行**二分查找** `lower_bound(cdf, cdf+n, x)`，找到第一个大于等于x 的下标位置 i，然后根据**插值**得到近似的 $cdf^{-1}(x)$

```cpp
// 线性插值
// 1.初始化cdf[]数组
int nbins = 32;  // 多少个柱子
float minBound = -5, maxBound = 5;  // 最小/大值
float dx = (maxBound - minBound) / nbins;
float cdf[nbins+1];  // cdf数组

// 2.填充cdf[]数组
cdf[0] = 0.f;
for (size_t n = 1; n < nbins; n++) {
    float x = minBound + dx * n, sum =0;
    float pdf_x = pdf(x) * dx;
    cdf[n] = cdf[n-1] + pdf_x;
    sum += pdf_x;
}
cdf[nbins] = 1.f;

// 3.随机采样点
float r = random.random();

// 4.线性插值
float *ptr = std::lower_bound(cdf, cdf + nbins + 1, r);
int off = (int)(ptr - cdf - 1);  // off为小于x对应的下标
// t为在当前柱子的占比
float t = (r - cdf[off]) / (cdf[off+1] - cdf[off]);
// x为cdf函数在整个x轴的范围的占比
float x = (off + t) / (float)(nbins);
// 插值得到最终结果
x = minBound * (1-x) + maxBound * x;
```

## 4. Ablation study 消融实验

可以看到，对实验结果影响最大的是 Viewing Direction 和 Pos Encoding，其次再到 Hierarchical

![[Pasted image 20230801171120.png]]

## 5. 缺陷与改进

1. 时间长：对于一张 1024 x 1024 的图片，每根光线需要先采样64个点，再采样128个点，遍历一张图片一次需要跑 1024 * 1024 * 64 + 1024 * 1024 * ( 64 + 128 ) ≈ 1e8 次网络
2. 泛化性差：对于新的场景需要重新跑一个网络
3. 需要大量视角？
4. 只针对静态场景？

## 6. 代码

代码主要围绕 `train()` 函数展开，且这里以lego数据集为样例

![[NeRF-2020 2023-08-05 19.32.42.excalidraw]]

下图是lego选取的不同的 `render_poses`

![[Pasted image 20230805104910.png|400]]


# 参考

NeRF：Representing Scenes as Neural Radiance Fields For View Synthesis

[Volume Rendering for Developers: Foundations (scratchapixel.com)](https://www.scratchapixel.com/lessons/3d-basic-rendering/volume-rendering-for-developers/intro-volume-rendering.html)

[大白话NeRF - 知乎 (zhihu.com)](https://www.zhihu.com/column/c_1591764824421261312)

[体渲染数学原理 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/56710440)

Volume Rendering Digest (for NeRF)

[NeRF代码解读-相机参数与坐标系变换 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/593204605)

[NeRF神经辐射场学习笔记（二）——Pytorch版NeRF实现以及代码注释_nerf pytorch_右边的口袋的博客-CSDN博客](https://blog.csdn.net/weixin_44292547/article/details/126249933)